{"cells":[{"metadata":{"id":"xgOiCyKpKO0c"},"cell_type":"markdown","source":"## Import Libraries","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# My idea\n\n\nWas get any knowledge about buying and selling houses, so i got some insights from this two websites: [8 Steps to Buying a House in Iowa](https://listwithclever.com/real-estate-blog/8-steps-to-buying-a-house-in-iowa/), [10 Important Features to Consider When Buying a House](https://homeia.com/10-important-features-to-consider-when-buying-a-house/) and [Will It Be a Seller's or Buyer's Market in 2019? Find Out Here](https://listwithclever.com/real-estate-blog/sellers-or-buyers-market/).\n\nAnd i learned about the US houses market, and wat it's important, when is needed to sell a house. I understand why the seasons it's imporant to sell a house, and the quality off the [appliance](https://homeia.com/10-important-features-to-consider-when-buying-a-house/#6-The-age-style-and-condition-of-home-appliances), and others factors.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"2S-bWOfCKO0g"},"cell_type":"code","source":"# My default libs\nimport numpy as np\nimport pandas as pd\n\n# Data viz libs\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\nimport plotly.graph_objs as go\n\n# Sklearn libs\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Models Libs\nfrom xgboost import XGBRegressor\n\n# Disable warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"GKxGPlfpKO0l"},"cell_type":"code","source":"# Files path\ntrain_data_path = '../input/house-prices-advanced-regression-techniques/train.csv'\ntest_data_path = '../input/house-prices-advanced-regression-techniques/test.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"4Z9Xb3JQKO0o"},"cell_type":"code","source":"# Getting the dataset's\ntrain_df = pd.read_csv(train_data_path)\ntest_df = pd.read_csv(test_data_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"J-Zq7ChDKO0r","outputId":"77dac37e-638f-4818-e72a-6e2766da6446"},"cell_type":"code","source":"# Look to shape's\nprint('Train dataset have ', train_df.shape[0], ' lines and ', train_df.shape[1], ' columns')\nprint('Test dataset have ', test_df.shape[0], ' lines and ', test_df.shape[1], ' columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"IV-kyiHnKO0u","outputId":"ab36710a-c8b7-4255-ceb8-6c1cc666b7cd"},"cell_type":"code","source":"# See the raw data, first 10 lines\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hypotheses\n\n- [X] The seasons don't impact the price.\n- [X] The lot area don't impact the price.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# EDA - Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## I create this new data frame, were i select the columns 'SalePrice' and 'MoSold'.\n\nThe idea is 'Use the month's to get the seasons and using boxplot, compare the distribuiton of the house's price's by seasons.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"year_seasons_df = train_df[['SalePrice','MoSold']].copy()\n\ndef setSeason(month):\n    if month in (6,7,8):\n        return \"Summer\"\n    if month in (11,10,9):\n        return \"Autumn\"\n    if month in (12,1,2):\n        return \"Winter\"\n    return \"Spring\"\n    \n\nyear_seasons_df['yearSeason'] = year_seasons_df.MoSold.apply(lambda x: setSeason(x));\n\nyear_seasons_df.sort_values(by='SalePrice', inplace=True)\n\ntrace = go.Box(\n    x = year_seasons_df.yearSeason,\n    y = year_seasons_df.SalePrice\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Prices x Year Season\",\n                  yaxis={'title':'Sale Price'},\n                  xaxis={'title':'Year Season'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"year_seasons_gp_df = year_seasons_df.groupby('yearSeason')['SalePrice'].count().reset_index()\n\nyear_seasons_gp_df = pd.DataFrame({'yearSeason': year_seasons_gp_df.yearSeason,\n                                   'CountHouse': year_seasons_gp_df.SalePrice})\n\nyear_seasons_gp_df.sort_values(by='CountHouse', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now i count how many houses was sold by year station","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"trace = go.Bar(\n    x = year_seasons_gp_df.yearSeason,\n    y = year_seasons_gp_df.CountHouse\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Count House x Year Station\",\n                  yaxis={'title':'Count House'},\n                  xaxis={'title':'Year Station'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conflict with the domain knowledge\n\nTake a look for the correlation by station, month with SalePrice, and for my surprise, the month and seasons dont have a higher correlation with the sale price.\n\nBut the **\"The seasons don't impact the price\"** hyphotesys was overturned hypothesis? No, becouse the diference of the houses prices between the seasons it's not so big, but it's ture, more house it's solded on the 'cold' seasons.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def labelSeason(x):\n    if x == \"Summer\":\n        return 1\n    if x == \"Autumn\":\n        return 2\n    if x == \"Winter\":\n        return 3\n    return 4\n\n\nyear_seasons_df['labelSeason'] = year_seasons_df.yearSeason.apply(lambda x: labelSeason(x))\n\ndf_corr_year_seasons = year_seasons_df.corr()\n\ndf_corr_year_seasons","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"year_seasons_sorted_df = year_seasons_df.sort_values(by='MoSold')\n\nyear_seasons_sorted_gp_df = year_seasons_df.groupby('MoSold')['SalePrice'].count().reset_index();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sales by month's\n\nHow many houes was sold by month ?","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = year_seasons_sorted_gp_df\n\ntrace = go.Scatter(\n    x = df.MoSold,\n    y = df.SalePrice,\n    mode = 'markers+lines',\n    line_shape='spline'\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Sales by month's\",\n                  yaxis={'title':'Count House'},\n                  xaxis={'title':'Month sold', 'zeroline':False})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lot Area by Price\n\nSee the distribution of houses price and the lot area, we can note 4 big's outliers, with a vast lot area","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trace = go.Scatter(\n    x = train_df.LotArea,\n    y = train_df.SalePrice,\n    mode = 'markers'\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Lot Area x Sale Price\",\n                  yaxis={'title':'Sale Price'},\n                  xaxis={'title':'Lot Area'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of Sale Price\n\nI used the boxplot to get some idea about the distribution, and we can note 2 outliers, prices greater than **600K**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"trace = go.Box(\n    y = train_df.SalePrice,\n    name = 'Sale Price'\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Distribuiton Sale Price\",\n                  yaxis={'title':'Sale Price'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of lot area\n\n*yes i love boxplot*\n\nIn the lot area the outliers it's the house with the lot area greater than **70k**\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"trace = go.Box(\n    y = train_df.LotArea,\n    name = 'Lot Area'\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Distribuiton Lot Area\",\n                  yaxis={'title':'Lot Area'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation by Lot Area and Price\n\n\nIt's a good correlation 0.26, so that's means the \"The lot area don't impact the price.\" hypotesys it's wrong, so the alternative hypothesys is 'The lot area impact in the house price' *(omg :o that's so unbelievable)*","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"lotarea_saleprice_df = train_df[['SalePrice', 'LotArea']]\n\nlotarea_saleprice_df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ..........................................................","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Removing Outiliers\n\n- According to sale price box plot, the dataset had some houses with values greater than 70000, so i removed this houses.\n- According to sale price by lot area scatter plot, the dataset had some lot area greater than 500000, so i removed this houses to.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(train_df.loc[(train_df['LotArea'] > 70000)].index)\ntrain_df = train_df.drop(train_df.loc[(train_df['SalePrice'] > 500000)].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### New distribution sale price by lot area","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trace = go.Scatter(\n    x = train_df.LotArea,\n    y = train_df.SalePrice,\n    mode = 'markers'\n)\n\ndata = [trace]\n\nlayout = go.Layout(title=\"Lot Area x Sale Price\",\n                  yaxis={'title':'Sale Price'},\n                  xaxis={'title':'Lot Area'})\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Targets and features\n\n\nSetting the target *y* = *Sale Price*\n\nSetting the features *X* = *Others columns* (?) ","execution_count":null},{"metadata":{"trusted":true,"id":"rD-vO0_PKO01"},"cell_type":"code","source":"# Set y (Target)\ny = np.log(train_df.SalePrice)\n\nX = train_df.copy()\n\nX_test = test_df.copy();","execution_count":null,"outputs":[]},{"metadata":{"id":"oyEKKFwBKO06"},"cell_type":"markdown","source":"# Feature engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I created the area util feature, my idea is 'Lot Area it's the house total area, and others it's non 'raw area', so i sum others area's and subtracted from the total area'. That's is like 'The main area'\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X['AreaUtil'] = X['LotArea'] - (X['MasVnrArea'] + X['GarageArea'] + X['PoolArea'])\nX_test['AreaUtil'] = X_test['LotArea'] - (X_test['MasVnrArea'] + X_test['GarageArea'] + X_test['PoolArea'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Have Pool it's a boolean feature, if the pool area it's greater than 0 means that house have a pool","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X['HavePool'] = X['PoolArea'] > 0\nX_test['HavePool'] = X_test['PoolArea'] > 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Polinominal idea\n\nIn the Andrew Ng course i learned about this [polinominal regression](https://towardsdatascience.com/introduction-to-linear-regression-and-polynomial-regression-f8adc96f31cb) and a get the idea about create a new squared features and that's features are the most positive correlation with the sale price. And the square root feature, i got from a netflix post (but i lost the link :c), were the post says 'square features it's good for linear regression' something like that, so i tryed it to.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X['GarageCars2'] = X['GarageCars']**2\nX['GarageCarsSQRT'] = np.sqrt(X['GarageCars'])\nX['GarageArea'] = X['GarageArea']**2\nX['GarageAreaSQRT'] = np.sqrt(X['GarageArea'])\nX['LotArea2'] = X['LotArea']**2\nX['LotAreaSQRT'] = np.sqrt(X['LotArea'])\nX['AreaUtil2'] = X['AreaUtil']**2\nX['AreaUtilSQRT'] = np.sqrt(X['AreaUtil'])\nX['GrLivArea2'] = X['GrLivArea']**2\nX['GrLivAreaSQRT'] = np.sqrt(X['GrLivArea'])\n\nX_test['GarageCars2'] = X_test['GarageCars']**2\nX_test['GarageCarsSQRT'] = np.sqrt(X_test['GarageCars'])\nX_test['GarageArea'] = X_test['GarageArea']**2\nX_test['GarageAreaSQRT'] = np.sqrt(X_test['GarageArea'])\nX_test['LotArea2'] = X_test['LotArea']**2\nX_test['LotAreaSQRT'] = np.sqrt(X_test['LotArea'])\nX_test['AreaUtil2'] = X_test['AreaUtil']**2\nX_test['AreaUtilSQRT'] = np.sqrt(X_test['AreaUtil'])\nX_test['GrLivArea2'] = X_test['GrLivArea']**2\nX_test['GrLivAreaSQRT'] = np.sqrt(X_test['GrLivArea'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Heatmap\n\nTake a look to correlations with the sale price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat = X.corr()\n\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0]\n\n# most correlated features\nif 1 == 1:\n    plt.figure(figsize=(30,15))\n    g = sns.heatmap(X[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"PKL2e-xAKO03"},"cell_type":"code","source":"# Remove row with missing target\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\n# Drop target.\nX.drop(['SalePrice'], axis=1, inplace=True)\n\nX.drop(['OverallQual'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cols_sem_muitos_dados = [col for col in X.columns if X[col].isnull().sum() * 100 / len(X) > 50.00]\n\ncols_sem_muitos_dados = [col for col in X.columns if X[col].isnull().any()]\n\nfor col in cols_sem_muitos_dados:\n    X[col].fillna(\"None\")\n    X_test[col].fillna(\"None\")\n\n#X.drop(cols_sem_muitos_dados, axis=1, inplace=True)\n#X_test.drop(cols_sem_muitos_dados, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"ikKmTWbSKO08","outputId":"d70d6988-4758-477e-ad44-85b75d5d2fc1"},"cell_type":"code","source":"print('Train Shape:', X.shape)\nprint('Test Shape:', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get categorical cols WITH type OBJECT (like string)\ncategorical_cols = [cname for cname in X.columns\nif X[cname].dtype == \"object\"            \n]\n\n# Get numerical cols\nnumerical_cols = [cname for cname in X.columns\nif X[cname].dtype in ['int64', 'float64']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"me9-19Y1KO0_","outputId":"b82c4d99-5850-4e33-fc0d-d53116b9c09f"},"cell_type":"code","source":"# Merge all cols\nmy_cols = categorical_cols + numerical_cols\nX = X[my_cols].copy()\nX_test = X_test[my_cols].copy()\n\n# Let's see the first 5 cols\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_cols = categorical_cols\n\nif 1 == 0:\n    \n    x_cat_unique_values  = [col for col in X[categorical_cols].columns if len(X[col].unique()) <= 10]\n\n    dict_diff_onehot = set(categorical_cols) - set(x_cat_unique_values)\n\n    one_hot_cols = x_cat_unique_values\n\n    labelEncoder = LabelEncoder()\n\n    for col in list(dict_diff_onehot):\n        x_unique = X[col].unique();\n        x_test_unique = X_test[col].unique();\n\n        union_uniques = list(x_unique) + list(x_test_unique)\n\n        uniques = list(dict.fromkeys(union_uniques));\n\n        labelEncoder.fit(uniques);\n\n        X[col] = labelEncoder.transform(X[col].astype(str))\n        X_test[col] = labelEncoder.transform(X_test[col].astype(str))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"cAH17iwbKO1F"},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import learning_curve\nfrom math import sqrt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"id":"JJD5i1wqKO1I"},"cell_type":"markdown","source":"# Pipeline","execution_count":null},{"metadata":{"trusted":true,"id":"ZMqDzsBxKO1I","outputId":"f1a10961-a41c-4b6b-b601-4311590d3ca7"},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"2V2rz2-EKO1K","outputId":"8948d780-7092-4c67-c6c5-5a2fe14b4aaf"},"cell_type":"code","source":"# Numerical data\nnumerical_transformer = Pipeline(steps=[\n  ('imputer', SimpleImputer(strategy='mean')),\n  ('scaler', MaxAbsScaler())\n])\n\n# Categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, one_hot_cols)\n    ]\n)\n\n# Bundle preprocessing and modeling the pipeline\npipeline = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor),\n    ]\n)\n\n# Preprocessing of training data\nX_train_fit = pipeline.fit_transform(X)\nX_test_fit = pipeline.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"How much features we have now =  \"+str(len(X_train_fit[0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 1 == 1:\n    from sklearn.feature_selection import RFE\n    \n    nfeature = 100\n    \n    trans = RFE(XGBRegressor(random_state=0,objective=\"reg:squarederror\"), n_features_to_select=nfeature)\n    \n    X_train_fit = trans.fit_transform(X_train_fit, y)\n    X_test_fit = trans.transform(X_test_fit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 1 == 1:\n    from sklearn.model_selection import RandomizedSearchCV\n   \n    try:\n        nfeature\n    except NameError:\n        nfeature = len(X_train_fit[0])\n        \n    n_estimators = range(100,2000,200)\n    max_depth = [2, 3, 5, 10, 15]\n    booster=['gbtree','gblinear','dart']\n    learning_rate=[0.05,0.1,0.012,0.013,0.015,0.016,0.020,0.025,0.2,0.3,0.5]\n    min_child_weight=[1,2,3,4]\n    base_score=[0.25,0.5,0.75,1]\n    max_features=range(1, nfeature)\n\n    # Define the grid of hyperparameters to search\n    hyperparameter_grid = {\n        'n_estimators': n_estimators,\n        'max_depth':max_depth,\n        'learning_rate':learning_rate,\n        'min_child_weight':min_child_weight,\n        'booster':booster,\n        'base_score':base_score,\n        'max_features':max_features\n        }\n\n    random_cv = RandomizedSearchCV(estimator=XGBRegressor(random_state=0,objective=\"reg:squarederror\"),\n                param_distributions=hyperparameter_grid,\n                cv=5, n_iter=30,\n                scoring = 'neg_mean_squared_error',n_jobs = 4,\n                verbose = 5,\n                return_train_score = False,\n                random_state=0)\n\n    random_cv.fit(X_train_fit,y)\n    \n    model = random_cv.best_estimator_\n    \n    print(random_cv.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 1 == 0:\n    model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0,\n             importance_type='gain', learning_rate=0.015, max_delta_step=0,\n             max_depth=3, max_features=97, min_child_weight=1, missing=None,\n             n_estimators=1100, n_jobs=1, nthread=None,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n             subsample=1, verbosity=1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(X_train_fit[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(5, shuffle=True, random_state=0)\n\nfor linhas_treino, linhas_valid in kf.split(X_train_fit):\n    X_train, X_valid = X_train_fit[linhas_treino], X_train_fit[linhas_valid];\n    y_train, y_valid = y.iloc[linhas_treino], y.iloc[linhas_valid];\n    \n    # Define Model\n    model.fit(X_train, y_train);\n\n    # Preprocessing of validation data, get predictions\n    \n    preds = model.predict(X_valid)\n    \n    print('MAE:', mean_absolute_error(np.exp(y_valid), np.exp(preds)),'\\n');\n    print('RMSE:', np.sqrt(mean_squared_error(y_valid, preds)),'\\n');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 1 == 1:\n    # Split data with validation data and train data\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train_fit, y, random_state = 0, train_size=0.8)\n    \n    model.fit(X_train, y_train);\n\n    # Preprocessing of validation data, get predictions\n    preds = model.predict(X_valid)\n\n    print('MAE:', mean_absolute_error(np.exp(y_valid), np.exp(preds)),'\\n');\n    print('RMSE:', np.sqrt(mean_squared_error(y_valid, preds)),'\\n');","execution_count":null,"outputs":[]},{"metadata":{"id":"FJX96Qb3KO1M"},"cell_type":"markdown","source":"## Finaly, prepare submit","execution_count":null},{"metadata":{"trusted":true,"id":"un-qMM1hKO1N","outputId":"17507528-8ecf-4f25-85b9-595315326acd"},"cell_type":"code","source":"preds_test = model.predict(X_test_fit)\n\n# Create OutPut Data\noutput = pd.DataFrame({'Id': X_test.Id, 'SalePrice': np.exp(preds_test)})\n\n# To CSV\noutput.to_csv('submission.csv', index=False)\n\n# Show me the data!\noutput.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}